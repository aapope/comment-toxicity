{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker.pytorch import PyTorch\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = sagemaker.Session()\n",
    "\n",
    "bucket = session.default_bucket()\n",
    "s3_prefix = 'capstone/v1'\n",
    "\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'processed'\n",
    "input_data = session.upload_data(path=data_dir, bucket=bucket, key_prefix=s3_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train_instance_count has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n",
      "train_instance_type has been renamed in sagemaker>=2.\n",
      "See: https://sagemaker.readthedocs.io/en/stable/v2.html for details.\n"
     ]
    }
   ],
   "source": [
    "output_path = f's3://{bucket}/{s3_prefix}/output'\n",
    "\n",
    "estimator = PyTorch(\n",
    "    source_dir='src',\n",
    "    entry_point='train.py',\n",
    "    role=role,\n",
    "    framework_version='1.1.0',\n",
    "    py_version='py3',\n",
    "    train_instance_count=1,\n",
    "    train_instance_type='ml.p2.xlarge',\n",
    "    output_path=output_path,\n",
    "    hyperparameters={\n",
    "        'seed': 1,\n",
    "        'batch-size': 512,\n",
    "        'epochs': 10,\n",
    "        'embedding-dim': 32,\n",
    "        'num-lstm-layers': 1,\n",
    "        'hidden-dims': 100,\n",
    "        # vocab size from previous step + 2 for\n",
    "        # out of vocab and empty\n",
    "        'vocab-size': 10002\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skipped in final run in favor of the hyperparameter tuning below\n",
    "estimator.fit({\n",
    "    'training': os.path.join(input_data, 'train'),\n",
    "    'eval': os.path.join(input_data, 'val')\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "........................................"
     ]
    }
   ],
   "source": [
    "from sagemaker.tuner import HyperparameterTuner, CategoricalParameter, IntegerParameter\n",
    "\n",
    "tuner = HyperparameterTuner(\n",
    "    estimator=estimator,\n",
    "    objective_metric_name='ROC_AUC',\n",
    "    objective_type='Maximize',\n",
    "    max_jobs=20,\n",
    "    max_parallel_jobs=2,\n",
    "    metric_definitions=[{\n",
    "        'Name': 'ROC_AUC',\n",
    "        'Regex': 'roc_auc: (.*?);'\n",
    "    }],\n",
    "    early_stopping_type='Auto',\n",
    "    hyperparameter_ranges={\n",
    "        'embedding-dim': CategoricalParameter([32, 64]),\n",
    "        'num-lstm-layers': IntegerParameter(1, 4),\n",
    "        'hidden-dims': CategoricalParameter(['100', '100 64', '100 64 32', '100 64 32 16'])\n",
    "    }\n",
    ")\n",
    "\n",
    "tuner.fit({\n",
    "    'training': os.path.join(input_data, 'train'),\n",
    "    'eval': os.path.join(input_data, 'val')\n",
    "})\n",
    "tuner.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.best_training_job()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = estimator.deploy(\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    initial_instance_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------!"
     ]
    }
   ],
   "source": [
    "# I had to pull the best model from the console because the notebook crashed \n",
    "# so I skipped the prior 2 cells\n",
    "from sagemaker.pytorch.model import PyTorchModel\n",
    "\n",
    "model_artifact_location = 's3://sagemaker-us-east-1-281832773096/capstone/v1/output/sagemaker-pytorch-210327-1846-005-c22ae97e/output/model.tar.gz'\n",
    "\n",
    "model = PyTorchModel(\n",
    "    model_data=model_artifact_location,\n",
    "    role=role,\n",
    "    source_dir='src',\n",
    "    entry_point='train.py',\n",
    "    framework_version='1.1.0',\n",
    "    py_version='py3',\n",
    ")\n",
    "\n",
    "predictor = model.deploy(\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    initial_instance_count=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: symspellpy in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (6.7.0)\n",
      "Requirement already satisfied: numpy>=1.13.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from symspellpy) (1.19.5)\n"
     ]
    }
   ],
   "source": [
    "! pip install symspellpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload  \n",
    "import src.utils\n",
    "reload(src.utils)\n",
    "import joblib\n",
    "with open('processed/vocab.joblib', 'rb') as f:\n",
    "    vocab = joblib.load(f)\n",
    "\n",
    "sentence = 'You bad, you are the worst person alive!!!'\n",
    "input_vec = src.utils.encode_single_input(sentence, vocab)\n",
    "response = predictor.predict(input_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.88455814, 0.00102099, 0.01581585, 0.01653206, 0.08217296,\n",
       "        0.01339703]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate with Final Test Set\n",
    "\n",
    "This section was only used on the final run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "with open('processed/test/data.joblib', 'rb') as f:\n",
    "    test_data = joblib.load(f)\n",
    "\n",
    "y_true = test_data[:, :6]\n",
    "X = test_data[:, 6:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = np.array([predictor.predict(chunk) for chunk in np.split(X, range(1000, X.shape[0], 1000))])\n",
    "y_pred = np.concatenate(y_pred, axis=0)\n",
    "y_pred_class = y_pred.round()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((40239, 6), (22355, 6))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred.shape, y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/ipykernel/__main__.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  if __name__ == '__main__':\n"
     ]
    }
   ],
   "source": [
    "with open('processed/y_pred.joblib', 'wb') as f:\n",
    "    joblib.dump(y_pred, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import src.utils\n",
    "\n",
    "full_df = pd.read_csv('data/raw_combined.csv')\n",
    "vocab, encoded_text = src.utils.encode_text(\n",
    "    full_df, \n",
    "    use_cache=True,\n",
    "    max_length=500,\n",
    "    vocab_length=10000\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FUCK YOU Ckatz you are german Cock sucker and FUCKER MOTHER FUCKER\n",
      "\n",
      "fuck <mvt> german cock sucker fucker mother fucker\n",
      "[0.997106   0.5659401  0.9982394  0.01774021 0.9745939  0.15446553]\n",
      "[1 0 1 0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# why is class 6, identity hate, so poorly performing?\n",
    "# pick a few false positives and false negatives\n",
    "\n",
    "from importlib import reload  \n",
    "import src.utils\n",
    "reload(src.utils)\n",
    "\n",
    "mask = (y_pred[:, 5] < 0.5) & (y_true[:, 5] == 1)\n",
    "false_neg = X[mask]\n",
    "false_neg_pred = y_pred[mask]\n",
    "false_neg_true = y_true[mask]\n",
    "\n",
    "idx = 10\n",
    "for i, row in enumerate(encoded_text):\n",
    "    if np.equal(row, false_neg[idx]).all():\n",
    "        print(full_df.iloc[i]['comment_text'])\n",
    "        break\n",
    "print()\n",
    "print(' '.join(src.utils.decode_text(false_neg[idx], vocab)))\n",
    "print(false_neg_pred[idx])\n",
    "print(false_neg_true[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, these comments are awful. But importantly there are lots of misspelled words here. Using a spelling correction tool in the preprocessing would likely help a lot."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
